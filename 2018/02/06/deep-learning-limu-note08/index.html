<!DOCTYPE html>
<html lang="zh-Hans">
<head>
  <meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=2">
<meta name="theme-color" content="#222">
<meta name="generator" content="Hexo 5.4.0">
  <link rel="apple-touch-icon" sizes="180x180" href="/Blog/images/apple-touch-icon-next.png">
  <link rel="icon" type="image/png" sizes="32x32" href="/Blog/images/favicon-32x32-next.png">
  <link rel="icon" type="image/png" sizes="16x16" href="/Blog/images/favicon-16x16-next.png">
  <link rel="mask-icon" href="/Blog/images/logo.svg" color="#222">

<link rel="stylesheet" href="/Blog/css/main.css">


<link rel="stylesheet" href="/Blog/lib/font-awesome/css/all.min.css">

<script id="hexo-configurations">
    var NexT = window.NexT || {};
    var CONFIG = {"hostname":"guoxs.github.io","root":"/Blog/","scheme":"Muse","version":"7.8.0","exturl":false,"sidebar":{"position":"left","display":"post","padding":18,"offset":12,"onmobile":false},"copycode":{"enable":false,"show_result":false,"style":null},"back2top":{"enable":true,"sidebar":false,"scrollpercent":false},"bookmark":{"enable":false,"color":"#222","save":"auto"},"fancybox":false,"mediumzoom":false,"lazyload":false,"pangu":false,"comments":{"style":"tabs","active":null,"storage":true,"lazyload":false,"nav":null},"algolia":{"hits":{"per_page":10},"labels":{"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}},"localsearch":{"enable":false,"trigger":"auto","top_n_per_article":1,"unescape":false,"preload":false},"motion":{"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}},"path":"search.xml"};
  </script>

  <meta name="description" content="语义分割一直是计算机视觉领域非常重要的研究方向，随着深度学习的发展，语义分割任务也得到了十足的进步, 本文从论文出发综述语义分割方法. 语义分割是指像素级的图像理解，即对图像中的每个像素标注所属的类别。示例图如下所示：  左图：输入图像；右图：图像的语义分割结果（源于：PASCAL VOC2011 Example SegmentationsPASCAL VOC2011 Example Segmen">
<meta property="og:type" content="article">
<meta property="og:title" content="MXNet&#x2F;Gluon 深度学习笔记 (八) —— 语义分割总结">
<meta property="og:url" content="http://guoxs.github.io/Blog/2018/02/06/deep-learning-limu-note08/index.html">
<meta property="og:site_name" content="Mooyu&#39;s Blog">
<meta property="og:description" content="语义分割一直是计算机视觉领域非常重要的研究方向，随着深度学习的发展，语义分割任务也得到了十足的进步, 本文从论文出发综述语义分割方法. 语义分割是指像素级的图像理解，即对图像中的每个像素标注所属的类别。示例图如下所示：  左图：输入图像；右图：图像的语义分割结果（源于：PASCAL VOC2011 Example SegmentationsPASCAL VOC2011 Example Segmen">
<meta property="og:locale">
<meta property="og:image" content="http://guoxs.github.io/Blog/2018/02/06/deep-learning-limu-note08/seg_pic.jpg">
<meta property="og:image" content="http://guoxs.github.io/Blog/2018/02/06/deep-learning-limu-note08/Unet.png">
<meta property="og:image" content="http://guoxs.github.io/Blog/2018/02/06/deep-learning-limu-note08/Atrous_conv.png">
<meta property="og:image" content="http://guoxs.github.io/Blog/2018/02/06/deep-learning-limu-note08/CRF.png">
<meta property="og:image" content="http://guoxs.github.io/Blog/2018/02/06/deep-learning-limu-note08/FCN-1.png">
<meta property="og:image" content="http://guoxs.github.io/Blog/2018/02/06/deep-learning-limu-note08/deconv01.gif">
<meta property="og:image" content="http://guoxs.github.io/Blog/2018/02/06/deep-learning-limu-note08/deconv02.gif">
<meta property="og:image" content="http://guoxs.github.io/Blog/2018/02/06/deep-learning-limu-note08/FCN-2.png">
<meta property="og:image" content="http://guoxs.github.io/Blog/2018/02/06/deep-learning-limu-note08/FCN-3.png">
<meta property="og:image" content="http://guoxs.github.io/Blog/2018/02/06/deep-learning-limu-note08/SegNet01.png">
<meta property="og:image" content="http://guoxs.github.io/Blog/2018/02/06/deep-learning-limu-note08/SegNet02.png">
<meta property="og:image" content="http://guoxs.github.io/Blog/2018/02/06/deep-learning-limu-note08/SegNet03.png">
<meta property="og:image" content="http://guoxs.github.io/Blog/2018/02/06/deep-learning-limu-note08/dilatedConv.gif">
<meta property="og:image" content="http://guoxs.github.io/Blog/2018/02/06/deep-learning-limu-note08/dilatedConv01.png">
<meta property="og:image" content="http://guoxs.github.io/Blog/2018/02/06/deep-learning-limu-note08/dilatedConv02.png">
<meta property="og:image" content="http://guoxs.github.io/Blog/2018/02/06/deep-learning-limu-note08/DeepLab01.png">
<meta property="og:image" content="http://guoxs.github.io/Blog/2018/02/06/deep-learning-limu-note08/DeepLab02.png">
<meta property="og:image" content="http://guoxs.github.io/Blog/2018/02/06/deep-learning-limu-note08/DeepLab03.png">
<meta property="og:image" content="http://guoxs.github.io/Blog/2018/02/06/deep-learning-limu-note08/DeepLab06.png">
<meta property="og:image" content="http://guoxs.github.io/Blog/2018/02/06/deep-learning-limu-note08/DeepLab04.png">
<meta property="og:image" content="http://guoxs.github.io/Blog/2018/02/06/deep-learning-limu-note08/DeepLab05.png">
<meta property="og:image" content="http://guoxs.github.io/Blog/2018/02/06/deep-learning-limu-note08/RefineNet.png">
<meta property="og:image" content="http://guoxs.github.io/Blog/2018/02/06/deep-learning-limu-note08/RefineNet02.png">
<meta property="og:image" content="http://guoxs.github.io/Blog/2018/02/06/deep-learning-limu-note08/RefineNet03.png">
<meta property="article:published_time" content="2018-02-06T09:00:16.000Z">
<meta property="article:modified_time" content="2021-12-12T12:33:57.202Z">
<meta property="article:author" content="Mooyu">
<meta property="article:tag" content="DeepLearning">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="http://guoxs.github.io/Blog/2018/02/06/deep-learning-limu-note08/seg_pic.jpg">

<link rel="canonical" href="http://guoxs.github.io/Blog/2018/02/06/deep-learning-limu-note08/">


<script id="page-configurations">
  // https://hexo.io/docs/variables.html
  CONFIG.page = {
    sidebar: "",
    isHome : false,
    isPost : true,
    lang   : 'zh-Hans'
  };
</script>

  <title>MXNet/Gluon 深度学习笔记 (八) —— 语义分割总结 | Mooyu's Blog</title>
  






  <noscript>
  <style>
  .use-motion .brand,
  .use-motion .menu-item,
  .sidebar-inner,
  .use-motion .post-block,
  .use-motion .pagination,
  .use-motion .comments,
  .use-motion .post-header,
  .use-motion .post-body,
  .use-motion .collection-header { opacity: initial; }

  .use-motion .site-title,
  .use-motion .site-subtitle {
    opacity: initial;
    top: initial;
  }

  .use-motion .logo-line-before i { left: initial; }
  .use-motion .logo-line-after i { right: initial; }
  </style>
</noscript>

<link rel="alternate" href="/Blog/atom.xml" title="Mooyu's Blog" type="application/atom+xml">
</head>

<body itemscope itemtype="http://schema.org/WebPage">
  <div class="container use-motion">
    <div class="headband"></div>

    <header class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-container">
  <div class="site-nav-toggle">
    <div class="toggle" aria-label="Toggle navigation bar">
      <span class="toggle-line toggle-line-first"></span>
      <span class="toggle-line toggle-line-middle"></span>
      <span class="toggle-line toggle-line-last"></span>
    </div>
  </div>

  <div class="site-meta">

    <a href="/Blog/" class="brand" rel="start">
      <span class="logo-line-before"><i></i></span>
      <h1 class="site-title">Mooyu's Blog</h1>
      <span class="logo-line-after"><i></i></span>
    </a>
  </div>

  <div class="site-nav-right">
    <div class="toggle popup-trigger">
    </div>
  </div>
</div>




<nav class="site-nav">
  <ul id="menu" class="main-menu menu">
        <li class="menu-item menu-item-home">

    <a href="/Blog/" rel="section"><i class="fa fa-home fa-fw"></i>Home</a>

  </li>
        <li class="menu-item menu-item-archives">

    <a href="/Blog/archives/" rel="section"><i class="fa fa-archive fa-fw"></i>Archives</a>

  </li>
  </ul>
</nav>




</div>
    </header>

    
  <div class="back-to-top">
    <i class="fa fa-arrow-up"></i>
    <span>0%</span>
  </div>


    <main class="main">
      <div class="main-inner">
        <div class="content-wrap">
          

          <div class="content post posts-expand">
            

    
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-Hans">
    <link itemprop="mainEntityOfPage" href="http://guoxs.github.io/Blog/2018/02/06/deep-learning-limu-note08/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/Blog/images/avatar.gif">
      <meta itemprop="name" content="Mooyu">
      <meta itemprop="description" content="Redefine everything">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Mooyu's Blog">
    </span>
      <header class="post-header">
        <h1 class="post-title" itemprop="name headline">
          MXNet/Gluon 深度学习笔记 (八) —— 语义分割总结
        </h1>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>

              <time title="Created: 2018-02-06 17:00:16" itemprop="dateCreated datePublished" datetime="2018-02-06T17:00:16+08:00">2018-02-06</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">Edited on</span>
                <time title="Modified: 2021-12-12 20:33:57" itemprop="dateModified" datetime="2021-12-12T20:33:57+08:00">2021-12-12</time>
              </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
        <p>语义分割一直是计算机视觉领域非常重要的研究方向，随着深度学习的发展，语义分割任务也得到了十足的进步, 本文从论文出发综述语义分割方法.</p>
<p>语义分割是指像素级的图像理解，即对图像中的每个像素标注所属的类别。示例图如下所示：</p>
<p><img src="/Blog/2018/02/06/deep-learning-limu-note08/seg_pic.jpg" alt="seg_pic"></p>
<p>左图：输入图像；右图：图像的语义分割结果（源于：<a href="http://host.robots.ox.ac.uk/pascal/VOC/voc2012/segexamples/index.html">PASCAL VOC2011 Example SegmentationsPASCAL VOC2011 Example Segmentations</a>）</p>
<p>除了识别图中的摩托车和车手外，我们还要标注每个目标的边界。因此，不同于图像分割，语义分割需要模型能够进行密集的像素级分类。</p>
<p>其中，<a href="http://host.robots.ox.ac.uk/pascal/VOC/voc2012/">VOC2012</a> 和 <a href="http://cocodataset.org/">MSCOCO</a> 是语义分割领域重要的数据集。</p>
<span id="more"></span>
<h2 id="语义分割方法"><a href="#语义分割方法" class="headerlink" title="语义分割方法"></a>语义分割方法</h2><p>在深度学习广泛应用于计算机视觉领域之前，人们一般使用 <a href="http://mi.eng.cam.ac.uk/~cipolla/publications/inproceedings/2008-CVPR-semantic-texton-forests.pdf">TextonForest</a> 和 <a href="http://www.cse.chalmers.se/edu/year/2011/course/TDA361/Advanced%20Computer%20Graphics/BodyPartRecognition.pdf">Random Forest based classifiers</a> 的方法进行语义分割。CNN（Convolutional Neural Network，卷积神经网络）不仅有助于图像识别，在图像的语义分割问题中同样取得了成功。</p>
<p>深度学习方法中常见的一种语义分割方法是 <a href="http://people.idsia.ch/~juergen/nips2012.pdf">图像块分类（patch classification）</a>，即利用像素周围的图像块对每一个像素进行分类。的原因是网络模型通常包含全连接层（fully connect layer），而且要求固定大小的图像输入。</p>
<p>2014 年，加州大学伯克利分校的 Long 等人提出 <a href="https://arxiv.org/abs/1411.4038">全卷积网络（Fully Convolutional Networks，FCN）</a>，使得卷积神经网络不需要全连接层就可以实现密集的像素级分类，从而成为当前非常流行的像素级分类 CNN 架构。由于不需要全连接层，所以可以对任意大小的图像进行语义分割，而且比传统方法要快上很多。之后，语义分割领域几乎所有的先进方法都是基于该模型进行扩展的。</p>
<p>除了全连接层，池化层（pooling layer）是 CNN 语义分割中的另一个主要问题。CNN 中池化层能够扩大感受野，丢弃位置信息（where information）从而聚合上下文信息。但是语义分割要求类别图完全贴合（exact alignment），因此需要保留位置信息。本文将介绍两种不同的分类架构解决这个问题。</p>
<p>一种是 <strong>编码器-解码器（encoder-decoder）</strong> 架构。编码器通过池化层逐渐减少空间维度，解码器则逐渐恢复物体的细节和空间维度。编码器到解码器之间通常存在快捷连接（shortcut connections），从而更好地恢复物体的细节信息。<a href="https://arxiv.org/pdf/1505.04597.pdf">U-Net</a> 是这类架构中最常用的模型之一。</p>
<p>U-Net: 编码器-解码器架构<br><img src="/Blog/2018/02/06/deep-learning-limu-note08/Unet.png" alt="Unet"></p>
<p>第二种这类架构使用 <a href="https://arxiv.org/pdf/1511.07122.pdf">空洞/带孔卷积（dilated/atrous convolutions）</a> 结构，从而去除池化层。</p>
<p><img src="/Blog/2018/02/06/deep-learning-limu-note08/Atrous_conv.png" alt="Atrous_conv"><br>Dilated/atrous卷积结构, 当rate=1时退化为普通的卷积结构.</p>
<p><a href="https://arxiv.org/pdf/1210.5644.pdf">条件随机场（Conditional Random Field，CRF）</a> 后处理操作通常用于进一步改善分割的效果。CRFs 是一种基于底层图像的像素强度进行“平滑”分割（‘smooth’ segmentation）的图模型，其工作原理是相似强度的像素更可能标记为同一类别。CRFs 一般能够提升 1-2% 的精度。</p>
<p><img src="/Blog/2018/02/06/deep-learning-limu-note08/CRF.png" alt="CRF"><br>CRF示意图。（b）一元分类结合CRF;（c, d, e）是CRF的变体，其中(e)是广泛使用的一种CRF。</p>
<p>接下来本文将概述从 FCN 以来一些具有代表性的图像语义分割的论文及其对应的架构，这些架构使用 <a href="http://host.robots.ox.ac.uk:8080/leaderboard/displaylb.php">VOC2012 评估服务器</a> 进行基准测试。</p>
<h2 id="FCN"><a href="#FCN" class="headerlink" title="FCN"></a>FCN</h2><blockquote>
<p>Fully Convolutional Networks for Semantic Segmentation</p>
<p>Submitted on 14 Nov 2014</p>
<p><a href="https://arxiv.org/abs/1411.4038">Arxiv Link</a></p>
</blockquote>
<p>这篇论文的主要思路是把 CNN 改成 FCN, 输入一幅图像后直接在输出端得到 dense prediction，也就是每个像素所属的 class，从而得到一个端到端的方法来实现图像的语义分割。</p>
<p>假设我们已经有了一个 CNN 模型, 首先要把 CNN 的全连接层看成是卷积层，卷积模板大小就是输入的特征 map 的大小，也就是说把全连接网络看成是对整张输入 map 做卷积。全连接层分别有 4096 个 6<em>6 的卷积核，4096 个 1</em>1 的卷积核，1000个1*1 的卷积核，如下图：</p>
<p><img src="/Blog/2018/02/06/deep-learning-limu-note08/FCN-1.png" alt="FCN-1"></p>
<p>接下来就要对这 1000 个 1<em>1 的输出，做 upsampling，得到 1000 个原图大小（如 32</em>32 ）的输出，这些输出合并后，得到上图所示的 heatmap 。</p>
<blockquote>
<p>这样做的好处是，能够很好的利用已经训练好的网络，不用像已有的方法那样，从头到尾训练，只需要微调即可，训练高效。</p>
</blockquote>
<h3 id="反卷积"><a href="#反卷积" class="headerlink" title="反卷积"></a>反卷积</h3><p>论文通过 upsampling 得到 dense prediction，文中提及到　3　种方案：</p>
<ul>
<li>shift-and-stitch: 设原图与 FCN 所得输出图之间的降采样因子是 f，那么对于原图的每个 f<em>f 的区域（不重叠）, 把这个 f</em>f 区域对应的 output 作为此时<strong>区域中心点像素</strong>对应的 output，这样就对每个 f*f 的区域得到了 $f^2$ 个 output，也就是每个像素都能对应一个 output，所以成为了 dense prediction;</li>
<li>filter rarefaction：就是放大 CNN 网络中的 subsampling 层的 filter 的尺寸，得到新的 filter. 公式如下, 其中 s 是 subsampling 的滑动步长，这个新 filter 的滑动步长要设为 1，这样的话，subsampling 就没有缩小图像尺寸，最后可以得到 dense prediction;<script type="math/tex; mode=display">
f'_{ij} =
\begin{cases}
f_{i/s, j/s},  & \text{if $s$ divides both $i$ and $j$} \\
0, & \text{otherwise}
\end{cases}</script></li>
<li>upsampling 的操作可以看成是反卷积(deconvolutional), 卷积运算的参数和CNN的参数一样是在训练FCN模型的过程中通过bp算法学习得到.</li>
</ul>
<blockquote>
<p>作者没有采用前两种方法, 而使用了最后一种反卷积的方法. 对于第一种方法, 虽然 receptive fileds 没有变小，但是由于原图被划分成 f*f 的区域输入网络，使得 filters 无法感受更精细的信息; 对于第二种方法, 下采样的功能被减弱，使得更细节的信息能被 filter 看到，但是 receptive fileds 会相对变小，可能会损失全局信息，且会对卷积层引入更多运算。</p>
</blockquote>
<p>反卷积示意图: 蓝色是反卷积层的input，绿色是反卷积层的output</p>
<p>kernel size = 3, stride = 1 的反卷积，input 是 2×2, output 是 4×4:</p>
<p><img src="/Blog/2018/02/06/deep-learning-limu-note08/deconv01.gif" alt="deconv01"></p>
<p>kernel size = 3, stride = 2 的反卷积，input 是 3×3, output 是 5×5：</p>
<p><img src="/Blog/2018/02/06/deep-learning-limu-note08/deconv02.gif" alt="deconv02"></p>
<blockquote>
<p>解卷积层也被称作：上卷积（upconvolution），完全卷积（full convolution），转置卷积（transposed convolution）或者微步卷积（fractionally-strided convolution）</p>
</blockquote>
<h3 id="fusion-prediction"><a href="#fusion-prediction" class="headerlink" title="fusion prediction"></a>fusion prediction</h3><p>由于池化操作造成的信息损失，上采样（即使采用解卷积操作）只能生成粗略的分割结果图。因此，论文从高分辨率的特征图中引入跳跃连接（shortcut/skip connection）操作改善上采样的精细程度：</p>
<p><img src="/Blog/2018/02/06/deep-learning-limu-note08/FCN-2.png" alt="FCN-2"></p>
<p>实验表明，这样的分割结果更细致更准确。在逐层fusion的过程中，做到第三行再往下，结果又会变差，所以作者做到这里就停了。可以看到如上三行的对应的结果：</p>
<p><img src="/Blog/2018/02/06/deep-learning-limu-note08/FCN-3.png" alt="FCN-3"></p>
<h3 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h3><ul>
<li>推广端到端卷积网络在语义分割领域中的应用</li>
<li>修改ImageNet预训练模型并应用于图像语义分割</li>
<li>采用解卷积层（deconvolutional layer）实现上采样</li>
<li>引入跳跃连接（skip connections）改善上采样的粒度（coarseness）</li>
</ul>
<blockquote>
<p>FCN 对于语义分割领域来说贡献巨大，但是它容易丢失较小的目标, 当前的方法已经取得了很大的提升。</p>
</blockquote>
<h2 id="SegNet"><a href="#SegNet" class="headerlink" title="SegNet"></a>SegNet</h2><blockquote>
<p>SegNet: A Deep Convolutional Encoder-Decoder Architecture for Image Segmentation</p>
<p>Submitted on 2 Nov 2015</p>
<p><a href="https://arxiv.org/abs/1511.00561">Arxiv Link</a></p>
</blockquote>
<p>尽管 FCN 网络中使用了解卷积层和少量跳跃连接，但输出的分割图比较粗略，因此本文引入更多的跳跃连接。但是，SegNet 并没有复制 FCN 中的编码器特征，取而代之的是复制最大池化索引。因此，SegNet 相对于 FCN 来说更节省内存。</p>
<p>SegNet 网络架构如下图所示:<br><img src="/Blog/2018/02/06/deep-learning-limu-note08/SegNet01.png" alt="SegNet"><br>SegNet 有三个部分构成：一个 encoder network，一个对应的 decoder network，最后一个像素级别的分类层.</p>
<p>编码器部分使用的是 VGG16 的前 13 层卷积网络，可以尝试使用 Imagenet 上的预训练. 还可以丢弃完全连接的层，有利于在最深的编码器输出处保留较高分辨率的特征图, 与其他最近的架构 FCN 和 DeconvNet 相比，这也减少了SegNet编码器网络中的参数数量. 每个编码器由卷积层、批归一化层、RELU 组成，之后，执行具有 2×2 窗口和步幅 2（非重叠窗口）的最大池化，输出结果相当于系数为 2 的下采样.最大池化用于实现输入图像中小空间位移的平移不变性，子采样导致特征图中每个像素的大输入图像上下文（空间窗口）. 由于最大池化和子采样的叠加，导致边界细节损失增大，因此必须在编码特征图中在 sub-sampling 之前捕获和储存边界信息. 为了高效，论文只储存了<strong>最大池化索引</strong>(max-pooling indices). 对于每个 2×2 池化窗口，这可以使用 2 位来完成，因此与浮动精度的记忆特征图相比，存储效率更高.</p>
<p>解码器网络中的解码器使用来自对应的编码器特征图存储的<strong>最大池化索引</strong>来上采样至其输入特征图. 此步骤产生稀疏特征图, 然后将这些特征图与可训练的解码器滤波器组卷积以产生密集的特征图. 注意，最后一个解码器产生一个多通道的特征图，而不是3通道的(RGB). 之后特征图输入给一个 softmax 分类器. 这个 softmax 独立地分类每个像素. softmax 分类器的输出是 K 通道图像的概率，其中 K 是类的数量, 预测的分割对应于在每个像素处具有最大概率的类.</p>
<p>使用<strong>最大池化索引</strong>的解码器示意图:<br><img src="/Blog/2018/02/06/deep-learning-limu-note08/SegNet02.png" alt="max-pooling indices"></p>
<p>SegNet 部分分割结果如下:<br><img src="/Blog/2018/02/06/deep-learning-limu-note08/SegNet03.png" alt="result"></p>
<h3 id="总结-1"><a href="#总结-1" class="headerlink" title="总结"></a>总结</h3><ul>
<li>FNC 和 SegNet 都是最早提出编码器-解码器结构的网络</li>
<li>分割的精度略好于 FCN, 总体效率也比 FCN 略高</li>
<li>SegNet 架构的基准测试分数一般，不建议继续使用</li>
</ul>
<h2 id="Dilated-Convolutions"><a href="#Dilated-Convolutions" class="headerlink" title="Dilated Convolutions"></a>Dilated Convolutions</h2><blockquote>
<p>Multi-Scale Context Aggregation by Dilated Convolutions</p>
<p>Submitted on 23 Nov 2015</p>
<p><a href="https://arxiv.org/abs/1511.07122">Arxiv Link</a></p>
</blockquote>
<p>在基于 FCN 思想的语义分割问题中，输出图像的 size 要和输入图像的 size 一致。但是 FCN 中由于有若干 stride&gt;1 的池化层，所以越到较高的网络层，单位像素中包含的原始图像的信息就越多，也就是感受野越大，但这是以通过池化降低分辨率、损失原始图像中的信息作为代价而得来的。由于 pooling 层的存在，后面层的 feature map 的 size 会越来越小，但由于需要计算 loss 等原因，最后输出图像的 size 要和输入图像的 size 保持一致，所以在 FCN 中的后段网络层中，必须对 feature map 进行<strong>上采样</strong>操作，将缩小的 feature map 再还原到原始尺寸，在这个过程中，不可能将在池化过程中丢失的信息完全还原回来，这样就造成了信息的丢失、语义分割的精度降低。</p>
<p>若不加 pooling 层，在较小的卷积核尺寸的前提下，感受野会很小；但如果为了增大感受野，在中段的网络层中使用 size 较大的卷积核，计算量又会暴增，内存扛不住, 因为中段的 channel 一般会非常大，比如 1024、2018，跟最开始 rgb 图像的 3 个 channel 比起来，增大了几百倍。</p>
<p>这种情况下， 作者提出空洞卷积层，其工作原理如下：</p>
<p><img src="/Blog/2018/02/06/deep-learning-limu-note08/dilatedConv.gif" alt="dilated conv"></p>
<p>Dilated Convolution 想法很粗暴，既然池化的下采样操作会带来信息损失，那么就把池化层去掉。但是池化层去掉随之带来的是网络各层的感受野变小，这样会降低整个模型的预测精度。Dilated convolution 的主要贡献就是，如何在去掉池化下采样操作的同时，而不降低网络的感受野。</p>
<h3 id="Dilated-Convolution"><a href="#Dilated-Convolution" class="headerlink" title="Dilated Convolution"></a>Dilated Convolution</h3><p>定义离散函数：$\mathbf{F}: \mathbb{Z}^2 \rightarrow \mathbb{R}$， 假设 $\Omega_r = [−r,r]^2 \bigcap \mathbb{Z}^2，k: \Omega_r \rightarrow \mathbb{R}$ 是大小为 $(2r+1)^2$ 的离散 filter. 则离散卷积操作 ∗ 的定义为：</p>
<script type="math/tex; mode=display">(F∗k)(p)=\sum_{s+t=p}F(s)k(t)</script><p>其一般化形式为：</p>
<script type="math/tex; mode=display">(F∗_{l}k)(p)=\sum_{s+lt=p}F(s)k(t)</script><p>其中 $l$ 为 dilation 因子，$∗_l$ 为 dilation convolution. 当 $l=1$ 时，即为普通的离散卷积操作 ∗.</p>
<p>基于 Dilation Convolution 的网络支持接受野的指数增长，不丢失分辨率信息.</p>
<p>记 $F<em>0,F_1,…,F</em>{n−1}: \mathbb{Z}^2 \rightarrow \mathbb{R}$ 为离散函数， $k<em>0,k_1,…, k</em>{n−2}: \Omega_1 \rightarrow \mathbb{R}$ 是离散的 3×3 fliters， 采用指数增长 dilation 的 filters后，</p>
<script type="math/tex; mode=display">F_{i+1} = F_i∗_{2^i}k_i \text{ , for } i=0,1,...,n−2</script><p>定义 $F<em>{i+1}$ 中的元素 p 的接受野为：$F_0$ 中可以改变 $F</em>{i+1}(p)$ 值的元素集. $F_{i+1}$ 中 p 的接受野的大小即为这些元素集的数目.</p>
<p>显而易见，$F_{i+1}$ 中各元素的接受野大小为 $(2^{i+2}−1)×(2^{i+2}−1)$. 接受野是指数增长大小的平方.</p>
<p>如下图.</p>
<p><img src="/Blog/2018/02/06/deep-learning-limu-note08/dilatedConv01.png" alt="dilated02"></p>
<p>以 $3 \times 3$ 的卷积核为例，传统卷积核在做卷积操作时，是将卷积核与输入张量中“连续”的 $3 \times 3$ 的 patch 逐点相乘再求和（如上图 a，红色圆点为卷积核对应的输入“像素”，绿色为其在原输入中的感知野）。而 dilated convolution 中的卷积核则是将输入张量的 $3\times 3$ patch 隔一定的像素进行卷积运算。如上图 b 所示，在去掉一层池化层后，需要在去掉的池化层后将传统卷积层换做一个 “dilation=2” 的 dilated convolution 层，此时卷积核将输入张量每隔一个“像素”的位置作为输入 patch 进行卷积计算，可以发现这时对应到原输入的感知野已经扩大（dilate）为 $7 \times 7$；同理，如果再去掉一个池化层，就要将其之后的卷积层换成 “dilation=4” 的 dilated convolution 层，如上图 c 所示。这样一来，即使去掉池化层也能保证网络的感受野，从而确保图像语义分割的精度。</p>
<blockquote>
<p>Dilated Convolution 能够不减少空间维度的前提下，使感受野呈现指数级增长。</p>
</blockquote>
<h3 id="Multi-scale-Context-Aggreation"><a href="#Multi-scale-Context-Aggreation" class="headerlink" title="Multi-scale Context Aggreation"></a>Multi-scale Context Aggreation</h3><p>上述模块在论文中称作前端模块（frontend module），使用上述模块之后，无需增加参数即可实现密集的像素级类别预测。另一个模块在论文中称作上下文模块（context module），使用前端模块的输出作为输入单独训练。该模块由多个不同扩张程度（dilation）的 dilated convolution 级联而成，因此能够聚合不同尺度的上下文信息，从而改善前端模块输出的预测结果。</p>
<p>context module 的结构如下：</p>
<p><img src="/Blog/2018/02/06/deep-learning-limu-note08/dilatedConv02.png" alt="dilated03"></p>
<p>context 模块的基本形式中，各层具有 C 个 channels. 尽管特征图没有归一化，模块内也没有定义 loss，但各层的表示是相同的，可以直接用于获得 dense per-class prediction. 直观上是可以增加特征图的准确度的.</p>
<p>基本的 context 模块有 7 层，各层采用具有不同的 dilation 因子的 3×3 卷积. 各卷积操作后跟着一个逐元素截断 (pointwise truncation) 操作：max(⋅,0). 最终的输出是采用 1×1×C 的卷积操作得到的.</p>
<h3 id="总结-2"><a href="#总结-2" class="headerlink" title="总结"></a>总结</h3><ul>
<li>采用空洞卷积（dilated convolution）作为能够实现像素级预测的卷积层</li>
<li>提出“背景模块”（context module），用于空洞卷积的多尺度聚合</li>
<li>预测分割图的大小是原始图大小的 1/8，几乎所有的方法都是这样，一般通过插值得到最终的分割结果</li>
</ul>
<h2 id="DeepLab-（v1-amp-v2）"><a href="#DeepLab-（v1-amp-v2）" class="headerlink" title="DeepLab （v1 &amp; v2）"></a>DeepLab （v1 &amp; v2）</h2><blockquote>
<p>Semantic Image Segmentation with Deep Convolutional Nets and Fully Connected CRFs</p>
<p>Submitted on 22 Dec 2014</p>
<p><a href="https://arxiv.org/abs/1412.7062">Arxiv Link</a></p>
<p>DeepLab: Semantic Image Segmentation with Deep Convolutional Nets, Atrous Convolution, and Fully Connected CRFs</p>
<p>Submitted on 2 Jun 2016</p>
<p><a href="https://arxiv.org/abs/1606.00915">Arxiv Link</a></p>
</blockquote>
<p>论文认为之前基于 FCN 的语义分割方法有如下几个问题：</p>
<ol>
<li>池化层和下采样使特征图的分辨率降低</li>
<li>多尺度物体检测</li>
<li>深度卷积神经网络的不变性造成的定位精度减少</li>
</ol>
<p>针对以上三个问题， DeepLab 分别提出了三个解决方案。</p>
<ol>
<li>针对分辨率降低的问题，DeepLab 提出使用 Atrous convolution, 这其实和上一个 dilated convolution 一样</li>
<li>传统方法是把 image 强行转成相同尺寸，这样会导致某些特征扭曲或者消失。作者的想法是借用<strong>空间金字塔池化</strong> (spatial pyramid pooling）的思想，使用了 ASPP (atrous SPP) 来解决这个问题</li>
<li>解决 DCNN invariance 可以选择挑过一些层，不过这里作者选择了全连接的 CRF 改善定位精度</li>
</ol>
<p>所以论文解决方案的整体流程如下：</p>
<p><img src="/Blog/2018/02/06/deep-learning-limu-note08/DeepLab01.png" alt="Deeplab 流程"></p>
<h3 id="用于稠密特征提取和视野增大的多孔卷积"><a href="#用于稠密特征提取和视野增大的多孔卷积" class="headerlink" title="用于稠密特征提取和视野增大的多孔卷积"></a>用于稠密特征提取和视野增大的多孔卷积</h3><p>对于重复使用卷积和池化操作导致特征图分辨率丢失的问题，作者采用了一个叫做 atrous convolution 的卷积操作，这和 dilated convolution 思想是一样的，这里不再做详细介绍。</p>
<p>Atrous convolution 结合了传统的 downsampling，convolution 和 upsampling，得到的图像大小相同，但是特征明显清晰了很多, 如下图所示：</p>
<p><img src="/Blog/2018/02/06/deep-learning-limu-note08/DeepLab02.png" alt="Atrous convolution"></p>
<p>上图展示了 2D 算法操作的一个简单例子。上排：低分辨率输入特征地图的标准卷积稀疏特征提取；下排：高分辨率输入特征地图多孔卷积比例 r=2 的稠密特征提取。</p>
<h3 id="用多孔空间金字塔池化的多尺度图像表示"><a href="#用多孔空间金字塔池化的多尺度图像表示" class="headerlink" title="用多孔空间金字塔池化的多尺度图像表示"></a>用多孔空间金字塔池化的多尺度图像表示</h3><p>对于第二个问题， 受 R-CNN 空间金字塔池化方法成功的启发，任意一个尺度上的区域都可以用单一尺度上重采样卷积特征进行精确有效地分类。论文使用多个不同采样率上的多个并行多孔卷积层，在每个采样率上提取特征然后再用单独的分支处理，融合生成最后的结果。如下图所示</p>
<p><img src="/Blog/2018/02/06/deep-learning-limu-note08/DeepLab03.png" alt="ASPP"></p>
<p>“多孔空间金字塔池化”（DeepLab-ASPP）方法可以泛化了 DeepLab-LargeFOV，如下图所示</p>
<p><img src="/Blog/2018/02/06/deep-learning-limu-note08/DeepLab06.png" alt="LargeFOV"></p>
<h3 id="用于精确边界恢复的全连接的条件随机场"><a href="#用于精确边界恢复的全连接的条件随机场" class="headerlink" title="用于精确边界恢复的全连接的条件随机场"></a>用于精确边界恢复的全连接的条件随机场</h3><p>精确定位和分类性能在神经网络中往往需要权衡：带有最大池化层的更深的模型在分类任务上非常成功，但不变性增加，顶层结点的大感受野只能产生平滑响应，这就使得深度卷积神经网络得分地图可以预测是否有物体，以及物体出现的大致位置，但不能真正描绘它们的边界。</p>
<p>论文将深度卷积神经网络的识别能力和全连接条件随机场优化的定位精度耦合在一起，非常成功地处理定位挑战问题，生成了精确的语义分割结果。</p>
<p>传统方法中，条件随机场用于平滑带噪声的分割图。这些模型将邻近结点耦合，有利于将相同标记分配给空间上接近的像素，这些短程条件随机场主函数会清除构建在局部手动特征上层弱分类器的错误预测。与这些弱分类器相比，现代深度卷积神经网络架构生成的得分地图通常非常平滑，在这种情形下，用短程条件随机场结果可能不理想。我们的目标是要恢复详细的局部结构而不是平滑它，用局部条件随机场关联中的反差灵敏势，可以增强定位，但还是会漏掉细小结构，通常都需要处理离散优化问题。为了克服短程条件随机场的局限，论文使用了全连接随机场模型。</p>
<p>使用 CRF 结果如下：</p>
<p><img src="/Blog/2018/02/06/deep-learning-limu-note08/DeepLab04.png" alt="CRF"></p>
<h3 id="总结-3"><a href="#总结-3" class="headerlink" title="总结"></a>总结</h3><p>论文部分结果展示：</p>
<p><img src="/Blog/2018/02/06/deep-learning-limu-note08/DeepLab05.png" alt="result"></p>
<p>主要贡献：</p>
<ul>
<li>采用了带孔/空洞卷积（atrous/dilated convolution）</li>
<li>提出了金字塔型的空洞池化（atrous spatial pyramid pooling，ASPP）</li>
<li>采用全连接的 CRF</li>
</ul>
<h2 id="RefineNet"><a href="#RefineNet" class="headerlink" title="RefineNet"></a>RefineNet</h2><blockquote>
<p>RefineNet: Multi-Path Refinement Networks for High-Resolution Semantic Segmentation</p>
<p>Submitted on 20 Nov 2016</p>
<p><a href="https://arxiv.org/abs/1611.06612">Arxiv Link</a></p>
</blockquote>
<p>采用空洞/带孔卷积的方法也存在缺点。空洞卷积需要大量的高分辨率特征图，因此对计算量和内存的消耗都很大，这也限制方法无法应用于高分辨率的精细预测。例如 DeepLab 中分割结果图采用原始输入图像的 1/8 大小。</p>
<p>因此，论文提出一种编码器-解码器的架构。编码器是 ResNet-101 模块，解码器则是 RefineNet 模块，其连接/融合编码器的高分辨率特征和先前 RefineNet 块中的低分辨率特征。从网络结构来看，该工作是 U-Net 的一个变种，文章的主要贡献和创新在于 U-Net 折返向上的通路之中。</p>
<p>论文提出的网络模型可以分为两段对应于 U-Net 中向下（特征逐步降采样同时提取语义特征）和向上（逐步上采样特征恢复细节信息）两段通路。其中向下的通路以 ResNet 为基础。向上的通路使用了新提出的 RefineNet 作为基础，并作为本通路特征与 ResNet 中低层特征的融合器。一个基本的框架如下图 (c) 所示：</p>
<p><img src="/Blog/2018/02/06/deep-learning-limu-note08/RefineNet.png" alt="RefineNet"></p>
<p>图a）代表的是标准的CNN结构<br>图b）代表的是带孔卷积dilated convolutions<br>图c）代表的是 RefineNet 的思路, 每一个小模块是一个 RefineNet, 融合了不同尺度下的 RefineNet 结果, 最终 upsample 到原图的 1/4 大小</p>
<h3 id="RefineNet-细节"><a href="#RefineNet-细节" class="headerlink" title="RefineNet 细节"></a>RefineNet 细节</h3><p>RefineNet 可以分为三个主要部分：</p>
<ol>
<li>不同尺度（也可能只有一个输入尺度）的特征输入首先经过两个 Residual 模块的处理；</li>
<li>之后是不同尺寸的特征进行融合。当然如果只有一个输入尺度，该模块则可以省去。所有特征上采样至最大的输入尺寸，然后进行加和。上采样之前的卷积模块是为了调整不同特征的数值尺度；</li>
<li>最后是一个链式的 pooling 模块。其设计本意是使用侧支上一系列的 pooling 来获取背景信息（通常尺寸较大）。直连通路上的 ReLU 可以在不显著影响梯度流通的情况下提高后续 pooling 的性能，同时不让网络的训练对学习率很敏感。<br>最后再经过一个 Residual 模块即得 RefineNet 的输出。</li>
</ol>
<p>如下图所示：<br><img src="/Blog/2018/02/06/deep-learning-limu-note08/RefineNet02.png" alt="RefineNet"></p>
<blockquote>
<p>RefineNet 的一个特点是使用了较多的 residual connection。这样的好处不仅在于在 RefineNet 内部形成了 short-range 的连接，对训练有益。此外还与 ResNet 形成了 long-range 的连接，让梯度能够有效传送到整个网络中。作者认为这一点对于网络是很有好处的。</p>
</blockquote>
<h3 id="几种不同的-RefineNet-设计"><a href="#几种不同的-RefineNet-设计" class="headerlink" title="几种不同的 RefineNet 设计"></a>几种不同的 RefineNet 设计</h3><p><img src="/Blog/2018/02/06/deep-learning-limu-note08/RefineNet03.png" alt="RefineNet"></p>
<h3 id="主要贡献"><a href="#主要贡献" class="headerlink" title="主要贡献"></a>主要贡献</h3><ul>
<li>编码器-解码器架构拥有精心设计的解码器模块</li>
<li>所有组件采用残差连接（residual connection）的设计</li>
</ul>
<h2 id="PSPNet"><a href="#PSPNet" class="headerlink" title="PSPNet"></a>PSPNet</h2><blockquote>
<p>Pyramid Scene Parsing Network</p>
<p>Submitted on 4 Dec 2016</p>
<p><a href="https://arxiv.org/abs/1612.01105">Arxiv Link</a></p>
</blockquote>
<h2 id="Large-Kernel-Matters"><a href="#Large-Kernel-Matters" class="headerlink" title="Large Kernel Matters"></a>Large Kernel Matters</h2><blockquote>
<p>Large Kernel Matters — Improve Semantic Segmentation by Global Convolutional Network</p>
<p>Submitted on 8 Mar 2017</p>
<p><a href="https://arxiv.org/abs/1703.02719">Arxiv Link</a></p>
</blockquote>
<h2 id="DeepLab-v3"><a href="#DeepLab-v3" class="headerlink" title="DeepLab v3"></a>DeepLab v3</h2><blockquote>
<p>Rethinking Atrous Convolution for Semantic Image Segmentation</p>
<p>Submitted on 17 Jun 2017</p>
<p><a href="https://arxiv.org/abs/1706.05587">Arxiv Link</a></p>
</blockquote>
<h2 id="参考文章"><a href="#参考文章" class="headerlink" title="参考文章"></a>参考文章</h2><p><a href="http://blog.qure.ai/notes/semantic-segmentation-deep-learning-review">A 2017 Guide to Semantic Segmentation with Deep Learning</a><br><a href="http://blog.csdn.net/u010025211/article/details/51209504">深度学习之图像分割-FCN</a><br><a href="http://blog.csdn.net/zziahgf/article/details/77947565">论文阅读理解 - Dilated Convolution</a><br><a href="http://blog.csdn.net/tonyyang1995/article/details/51915968">DeepLab: Semantic Image Segmentation</a><br><a href="https://zhuanlan.zhihu.com/p/22308032">【总结】图像语义分割之FCN和CRF</a><br><a href="http://blog.csdn.net/u010158659/article/details/72352833">RefineNet: Multi-Path Refinement Networks for High-Resolution Semantic</a></p>

    </div>

    
    
    

      <footer class="post-footer">
          <div class="post-tags">
              <a href="/Blog/tags/DeepLearning/" rel="tag"># DeepLearning</a>
          </div>

        


        
    <div class="post-nav">
      <div class="post-nav-item">
    <a href="/Blog/2018/02/06/deep-learning-limu-note07/" rel="prev" title="MXNet/Gluon 深度学习笔记 (七) —— 样式迁移">
      <i class="fa fa-chevron-left"></i> MXNet/Gluon 深度学习笔记 (七) —— 样式迁移
    </a></div>
      <div class="post-nav-item">
    <a href="/Blog/2018/02/16/deep-learning-limu-note09/" rel="next" title="MXNet/Gluon 深度学习笔记 (九) —— 循环神经网络">
      MXNet/Gluon 深度学习笔记 (九) —— 循环神经网络 <i class="fa fa-chevron-right"></i>
    </a></div>
    </div>
      </footer>
    
  </article>
  
  
  



          </div>
          

<script>
  window.addEventListener('tabs:register', () => {
    let { activeClass } = CONFIG.comments;
    if (CONFIG.comments.storage) {
      activeClass = localStorage.getItem('comments_active') || activeClass;
    }
    if (activeClass) {
      let activeTab = document.querySelector(`a[href="#comment-${activeClass}"]`);
      if (activeTab) {
        activeTab.click();
      }
    }
  });
  if (CONFIG.comments.storage) {
    window.addEventListener('tabs:click', event => {
      if (!event.target.matches('.tabs-comment .tab-content .tab-pane')) return;
      let commentClass = event.target.classList[1];
      localStorage.setItem('comments_active', commentClass);
    });
  }
</script>

        </div>
          
  
  <div class="toggle sidebar-toggle">
    <span class="toggle-line toggle-line-first"></span>
    <span class="toggle-line toggle-line-middle"></span>
    <span class="toggle-line toggle-line-last"></span>
  </div>

  <aside class="sidebar">
    <div class="sidebar-inner">

      <ul class="sidebar-nav motion-element">
        <li class="sidebar-nav-toc">
          Table of Contents
        </li>
        <li class="sidebar-nav-overview">
          Overview
        </li>
      </ul>

      <!--noindex-->
      <div class="post-toc-wrap sidebar-panel">
          <div class="post-toc motion-element"><ol class="nav"><li class="nav-item nav-level-2"><a class="nav-link" href="#%E8%AF%AD%E4%B9%89%E5%88%86%E5%89%B2%E6%96%B9%E6%B3%95"><span class="nav-number">1.</span> <span class="nav-text">语义分割方法</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#FCN"><span class="nav-number">2.</span> <span class="nav-text">FCN</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#%E5%8F%8D%E5%8D%B7%E7%A7%AF"><span class="nav-number">2.1.</span> <span class="nav-text">反卷积</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#fusion-prediction"><span class="nav-number">2.2.</span> <span class="nav-text">fusion prediction</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E6%80%BB%E7%BB%93"><span class="nav-number">2.3.</span> <span class="nav-text">总结</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#SegNet"><span class="nav-number">3.</span> <span class="nav-text">SegNet</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#%E6%80%BB%E7%BB%93-1"><span class="nav-number">3.1.</span> <span class="nav-text">总结</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Dilated-Convolutions"><span class="nav-number">4.</span> <span class="nav-text">Dilated Convolutions</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#Dilated-Convolution"><span class="nav-number">4.1.</span> <span class="nav-text">Dilated Convolution</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Multi-scale-Context-Aggreation"><span class="nav-number">4.2.</span> <span class="nav-text">Multi-scale Context Aggreation</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E6%80%BB%E7%BB%93-2"><span class="nav-number">4.3.</span> <span class="nav-text">总结</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#DeepLab-%EF%BC%88v1-amp-v2%EF%BC%89"><span class="nav-number">5.</span> <span class="nav-text">DeepLab （v1 &amp; v2）</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#%E7%94%A8%E4%BA%8E%E7%A8%A0%E5%AF%86%E7%89%B9%E5%BE%81%E6%8F%90%E5%8F%96%E5%92%8C%E8%A7%86%E9%87%8E%E5%A2%9E%E5%A4%A7%E7%9A%84%E5%A4%9A%E5%AD%94%E5%8D%B7%E7%A7%AF"><span class="nav-number">5.1.</span> <span class="nav-text">用于稠密特征提取和视野增大的多孔卷积</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E7%94%A8%E5%A4%9A%E5%AD%94%E7%A9%BA%E9%97%B4%E9%87%91%E5%AD%97%E5%A1%94%E6%B1%A0%E5%8C%96%E7%9A%84%E5%A4%9A%E5%B0%BA%E5%BA%A6%E5%9B%BE%E5%83%8F%E8%A1%A8%E7%A4%BA"><span class="nav-number">5.2.</span> <span class="nav-text">用多孔空间金字塔池化的多尺度图像表示</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E7%94%A8%E4%BA%8E%E7%B2%BE%E7%A1%AE%E8%BE%B9%E7%95%8C%E6%81%A2%E5%A4%8D%E7%9A%84%E5%85%A8%E8%BF%9E%E6%8E%A5%E7%9A%84%E6%9D%A1%E4%BB%B6%E9%9A%8F%E6%9C%BA%E5%9C%BA"><span class="nav-number">5.3.</span> <span class="nav-text">用于精确边界恢复的全连接的条件随机场</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E6%80%BB%E7%BB%93-3"><span class="nav-number">5.4.</span> <span class="nav-text">总结</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#RefineNet"><span class="nav-number">6.</span> <span class="nav-text">RefineNet</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#RefineNet-%E7%BB%86%E8%8A%82"><span class="nav-number">6.1.</span> <span class="nav-text">RefineNet 细节</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E5%87%A0%E7%A7%8D%E4%B8%8D%E5%90%8C%E7%9A%84-RefineNet-%E8%AE%BE%E8%AE%A1"><span class="nav-number">6.2.</span> <span class="nav-text">几种不同的 RefineNet 设计</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E4%B8%BB%E8%A6%81%E8%B4%A1%E7%8C%AE"><span class="nav-number">6.3.</span> <span class="nav-text">主要贡献</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#PSPNet"><span class="nav-number">7.</span> <span class="nav-text">PSPNet</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Large-Kernel-Matters"><span class="nav-number">8.</span> <span class="nav-text">Large Kernel Matters</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#DeepLab-v3"><span class="nav-number">9.</span> <span class="nav-text">DeepLab v3</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E5%8F%82%E8%80%83%E6%96%87%E7%AB%A0"><span class="nav-number">10.</span> <span class="nav-text">参考文章</span></a></li></ol></div>
      </div>
      <!--/noindex-->

      <div class="site-overview-wrap sidebar-panel">
        <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
  <p class="site-author-name" itemprop="name">Mooyu</p>
  <div class="site-description" itemprop="description">Redefine everything</div>
</div>
<div class="site-state-wrap motion-element">
  <nav class="site-state">
      <div class="site-state-item site-state-posts">
          <a href="/Blog/archives/">
        
          <span class="site-state-item-count">35</span>
          <span class="site-state-item-name">posts</span>
        </a>
      </div>
      <div class="site-state-item site-state-tags">
            <a href="/Blog/tags/">
        <span class="site-state-item-count">8</span>
        <span class="site-state-item-name">tags</span></a>
      </div>
  </nav>
</div>



      </div>

    </div>
  </aside>
  <div id="sidebar-dimmer"></div>


      </div>
    </main>

    <footer class="footer">
      <div class="footer-inner">
        

        

<div class="copyright">
  
  &copy; 
  <span itemprop="copyrightYear">2021</span>
  <span class="with-love">
    <i class="fa fa-heart"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">Mooyu</span>
</div>
  <div class="powered-by">Powered by <a href="https://hexo.io/" class="theme-link" rel="noopener" target="_blank">Hexo</a> & <a href="https://muse.theme-next.org/" class="theme-link" rel="noopener" target="_blank">NexT.Muse</a>
  </div>

        








      </div>
    </footer>
  </div>

  
  <script src="/Blog/lib/anime.min.js"></script>
  <script src="/Blog/lib/velocity/velocity.min.js"></script>
  <script src="/Blog/lib/velocity/velocity.ui.min.js"></script>

<script src="/Blog/js/utils.js"></script>

<script src="/Blog/js/motion.js"></script>


<script src="/Blog/js/schemes/muse.js"></script>


<script src="/Blog/js/next-boot.js"></script>




  















  

  
      

<script>
  if (typeof MathJax === 'undefined') {
    window.MathJax = {
      loader: {
        source: {
          '[tex]/amsCd': '[tex]/amscd',
          '[tex]/AMScd': '[tex]/amscd'
        }
      },
      tex: {
        inlineMath: {'[+]': [['$', '$']]},
        tags: 'ams'
      },
      options: {
        renderActions: {
          findScript: [10, doc => {
            document.querySelectorAll('script[type^="math/tex"]').forEach(node => {
              const display = !!node.type.match(/; *mode=display/);
              const math = new doc.options.MathItem(node.textContent, doc.inputJax[0], display);
              const text = document.createTextNode('');
              node.parentNode.replaceChild(text, node);
              math.start = {node: text, delim: '', n: 0};
              math.end = {node: text, delim: '', n: 0};
              doc.math.push(math);
            });
          }, '', false],
          insertedScript: [200, () => {
            document.querySelectorAll('mjx-container').forEach(node => {
              let target = node.parentNode;
              if (target.nodeName.toLowerCase() === 'li') {
                target.parentNode.classList.add('has-jax');
              }
            });
          }, '', false]
        }
      }
    };
    (function () {
      var script = document.createElement('script');
      script.src = '//cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js';
      script.defer = true;
      document.head.appendChild(script);
    })();
  } else {
    MathJax.startup.document.state(0);
    MathJax.texReset();
    MathJax.typeset();
  }
</script>

    

  

</body>
</html>
